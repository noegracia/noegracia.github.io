[{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/basic/_index.bn/","summary":"","tags":null,"title":"Go বেসিক"},{"categories":null,"contents":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/basic/introduction/","summary":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","tags":null,"title":"Introduction"},{"categories":null,"contents":" Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.Println(\u0026#34;Value is\u0026#34;, b) func getPointer () (myPointer *int) { a := 234 return \u0026amp;a a := new(int) *a = 234 Pointers point to a memory location of a variable. Go is fully garbage-collected.\nType Conversion i := 2 f := float64(i) u := uint(i) Slice slice := []int{2, 3, 4} slice := []byte(\u0026#34;Hello\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/basic/types/","summary":"Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.","tags":null,"title":"Basic Types"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/advanced/_index.bn/","summary":"","tags":null,"title":"অ্যাডভান্সড"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.Printf(\u0026#34;At position %d, the character %s is present\\n\u0026#34;, i, val) n := 0 x := 42 for n != x { n := guess() } ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/basic/flow-control/","summary":"Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.","tags":null,"title":"Flow Control"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/advanced/files/","summary":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","tags":null,"title":"File Manipulation"},{"categories":null,"contents":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/bash/basic/","summary":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","tags":null,"title":"Bash Variables"},{"categories":["Life"],"contents":"Ascending Aneto: Conquering the Pyrenees\u0026rsquo;s Pinnacle The adventure began with anticipation as we gathered our gear, crampons and ice axes, essential for the challenge ahead. From the starting elevation of 1700 meters, we embarked on our ascent to 2140 meters, finding sanctuary at the Renclusa Refuge. Dinner was our prelude to rest, yet sleep proved elusive. At 9 PM, I nestled into my bunk, but the altitude, the cold, and a symphony of nocturnal noises (someone snoring) allowed for only two hours of light slumber.\nAs I wake up at 6 AM, I had breakfast and then started our climbing non stop. For us novices, the path was not without its confusions; a few missteps cost us time but we could find our path by following experienced people that also were climbing the mountain. We trekked tirelessly until 1 PM, when we reached the 3000-meter mark and attempted to refuel. Altitude induced a sense of anorexia, making each bite of our sandwiches a laborious task. I managed only two-thirds of mine, despite usually having a robust appetite.\nIt was here that one of our friends, suffering from fatigue and having mountain sickness, chose to stay behind. The rest of us, determined, pressed on towards Aneto\u0026rsquo;s zenith. Seven hours of climbing rewarded us with a panoramic spectacle, we had beauty in every direction. The ascent was arduous, but the inspiring views at every altitude, from the valley\u0026rsquo;s charm at the base to the alpenglow of sunset painting the sky during our descent, were the true treasures of the journey.\nOur descent back to the car at 1700 meters marked the culmination of a 12-hour odyssey, not counting the previous night\u0026rsquo;s efforts. It was a 12-hour journey of traversing snowfields, dragging our legs through the snow, climbing, having lack of oxygen, falling in crevasses (hopefully not very deep ones). This grueling 15-hour journey was probably one of the most demanding physical feats I have ever undertaken.\nPhoto at the top of the mountain :P\n","date":"February 5, 2024","hero":"/posts/my-life/mountain/aneto/subida.jpg","permalink":"https://noegracia.github.io/posts/my-life/mountain/aneto/","summary":"Ascending Aneto: Conquering the Pyrenees\u0026rsquo;s Pinnacle The adventure began with anticipation as we gathered our gear, crampons and ice axes, essential for the challenge ahead. From the starting elevation of 1700 meters, we embarked on our ascent to 2140 meters, finding sanctuary at the Renclusa Refuge. Dinner was our prelude to rest, yet sleep proved elusive. At 9 PM, I nestled into my bunk, but the altitude, the cold, and a symphony of nocturnal noises (someone snoring) allowed for only two hours of light slumber.","tags":["life","routine"],"title":"Aneto"},{"categories":["Life"],"contents":"My Daily Routine Sports and Physical Activities Every day, I make sure to prioritize my physical well-being. My routine includes approximately 30 minutes of biking, which can vary depending on whether I\u0026rsquo;m working remotely or other factors. According to the World Health Organisation adults aged between 18-64 years should do at least 150–300 minutes of moderate-intensity aerobic physical activity, so I always like to do some cardio, moreover it makes me feel good. Apart from cardio I also like to do muscle strengthening. I usually do push pull legs workouts twice a week, whether it\u0026rsquo;s at the gym or at street workout parks. However, I also like doing diverse physical activities so I usually skip my workout days to do sports like tennis, volleyball, football, and even dance. My workouts are enough intense to progress but not too intense to be able to stick to my routine without being fatigued.\nMeal Preps Nutrition is a crucial part of maintaining a balanced lifestyle. I\u0026rsquo;m a dedicated meal prepper, ensuring that I have a nutritious meal ready for the next day to avoid long meal breaks. My meal prep is pretty simple. It always includes 3 important components: protein sources like chicken, salmon, or beef, coupled with carbohydrates like rice or pasta, and vegetables. On weekends, I allow myself to be more flexible, enjoying restaurant meals. I eat a lot of fruits, my favorites being bananas, kiwis and tangerines in season.\nSleep A good night\u0026rsquo;s sleep is non-negotiable in my routine. I aim for a solid 7-8 hours of rest every night. To ensure a peaceful night\u0026rsquo;s sleep, I have a calming bedtime routine. I brush my teeths, do my skincare and then I like to wind down with a good book; currently, I\u0026rsquo;m immersed in Brandon Sanderson\u0026rsquo;s \u0026ldquo;The Final Empire\u0026rdquo;. I make sure to close all windows and shut off every light, creating the perfect sleep environment.\n","date":"August 7, 2023","hero":"/posts/my-life/about-me/my-routine/velo.jpg","permalink":"https://noegracia.github.io/posts/my-life/about-me/my-routine/","summary":"My Daily Routine Sports and Physical Activities Every day, I make sure to prioritize my physical well-being. My routine includes approximately 30 minutes of biking, which can vary depending on whether I\u0026rsquo;m working remotely or other factors. According to the World Health Organisation adults aged between 18-64 years should do at least 150–300 minutes of moderate-intensity aerobic physical activity, so I always like to do some cardio, moreover it makes me feel good.","tags":["life","routine"],"title":"My routine"},{"categories":["Life"],"contents":"Setup My coding setup consists of a dual-monitor arrangement and a high-performance PC featuring a GeForce 3080, an i7-12700KF, 64GB RAM 3200MHz, and a 2TB SSD. This configuration allows me to efficiently handle large datasets and run resource-intensive algorithms, enhancing my productivity.\n","date":"August 6, 2023","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/posts/my-life/about-me/my-setup/","summary":"Setup My coding setup consists of a dual-monitor arrangement and a high-performance PC featuring a GeForce 3080, an i7-12700KF, 64GB RAM 3200MHz, and a 2TB SSD. This configuration allows me to efficiently handle large datasets and run resource-intensive algorithms, enhancing my productivity.","tags":["pc"],"title":"My setup"},{"categories":null,"contents":" Contents download Practical session files can be downloaded through your course on Moodle, or at this link.\nGoal description This practical session aims at performing 3D segmentation and shape checking on polyhedral objects, i.e. checking if these objects are correct with respect to a reference geometrical model and/or have defects (holes, residues, etc.).\nTo do this, we must first build this reference model from an RGB-D image of a defect-free object. Then, for any view of an unknown (also called \u0026ldquo;test\u0026rdquo;) object, we must segment it from the background and compare it with the reference model. This process of checking the shape must be independent of the viewpoint and, This shape verification process must be viewpoint independent and therefore requires the registration of each associated point cloud against the reference one.\nWe propose to break this objective down into three steps:\nstep 1 : extract the 3D points of the objects to be compared (both reference and test objects) by deleting all the points of the scene not belonging to the object. To avoid a redundant process, this step will be performed only on the reference scene contained in data01.xyz; this has already been done on the objects to be tested, and stored in the files data02_object.xyz and data03_object.xyz. step 2 : register the points of each test object to the reference model in order to compare them i.e. align their respective 3D point clouds to the reference coordinate frame. step 3 : compare the control and reference models and conclude on the potential flaws of the control models. Step 1: 3D model extraction from the reference scene The first step of the practical session consists in extracting the point cloud of the reference model from the RGB-D scene acquired with a Kinect :\nThis step aims at tracing a plane surface on the ground plane, and only keeping the center box by calculating the distance of each of its points from this plane, before applying a filtering threshold.\nTo do this, open CloudCompare (the main program, not the viewer) and import the points of the data01.xyz scene. Select the cloud by clicking on it in the workspace. Using the segmentation tool (Edit \u0026gt; Segment, or directly the \u0026ldquo;scissors\u0026rdquo; shortcut in the shortcut bar), divide the cloud into three subsets in order to extract the ground plane and a rough area around the box. The result is shown in the following figure:\nIn CloudCompare, to work on a point cloud, the corresponding line must be selected in the workspace. You know if the cloud is selected when a yellow box is displayed around it.\nChecking the box does not select the cloud, it simply makes it visible/invisible in the display.\nCreate a surface fitting the ground plane cloud using the Tools \u0026gt; Fit \u0026gt; Plane tool. By selecting the newly created plane and the cloud that contains the box, it is now possible to calculate, for each of the points of this cloud, its distance to the plane using the Tools \u0026gt; Distances \u0026gt; Cloud/Mesh Distance tool:\nThe distance tool adds a fourth field to each point of the cloud: the newly calculated distance. Using the cloud properties, filter the points with respect to this scalar field to keep only the points belonging to the box :\nBy clicking on split, two clouds are created, corresponding to the two sides of the filter:\nMake sure that the newly created cloud contains about 10,000 points (the number of points is accessible in the properties panel on the left).\nOnly select the box cloud before saving it in ASCII Cloud format as data01_segmented.xyz in your data folder.\nAs a precaution, save your CloudCompare project: remember to select all point clouds, and save the project in CloudCompare format. Step 2: 3D points registration If you have opened the complete scenes data02.xyz and data03.xyz in CloudCompare, you will have noticed that each scene was taken from a slightly different point of view, and that the objects themselves have moved:\nIn order to compare the models between them, we propose to overlay them and to calculate their cumulative distance point to point. The smaller this distance, the more the models overlap and resemble each other; the larger it is, the more the models differ. The following example shows the superposition of the correct model on the previously extracted reference model:\nTransforming the points of a model via a rotation/translation matrix to overlay it on another cloud is called point registration. The Iterative Closest Point algorithm allows this registration, and we propose to use it in Python. The code to be modified is only in qualitycheck.py, the goal being to apply ICP on both the correct model data02_object.xyz, and on the faulty model data03_object.xyz.\nLoading models The first part of the code loads the .xyz models extracted with CloudCompare, stores the reference model in the ref variable and the model to be compared in the data variable. To run the code on either data02_object or data03_object, just comment out the corresponding line.\n# Load pre-processed model point cloud print(\u0026#34;Extracting MODEL object...\u0026#34;) model = datatools.load_XYZ_data_to_vec(\u0026#39;data/data01_segmented.xyz\u0026#39;)[:,:3] # Load raw data point cloud print(\u0026#34;Extracting DATA02 object...\u0026#34;) data02_object = datatools.load_XYZ_data_to_vec(\u0026#39;data/data02_object.xyz\u0026#39;) # Load raw data point cloud print(\u0026#34;Extracting DATA03 object...\u0026#34;) data03_object = datatools.load_XYZ_data_to_vec(\u0026#39;data/data03_object.xyz\u0026#39;) ref = model_object data = data02_object # data = data03_object ICP call The second part of the code consists in coding the call to the icp function of the icp library\u0026hellip;\n########################################################################## # Call ICP: # Here you have to call the icp function in icp library, get its return # variables and apply the transformation to the model in order to overlay # it onto the reference model. matrix = np.eye(4,4) # Transformation matrix returned by icp function errors = np.zeros((1,100)) # Error value for each iteration of ICP iterations = 100 # The total number of iterations applied by ICP total_time=0 # Total time of convergence of ICP # ------- YOUR TURN HERE -------- # Draw results fig = plt.figure(1, figsize=(20, 5)) ax = fig.add_subplot(131, projection=\u0026#39;3d\u0026#39;) # Draw reference datatools.draw_data(ref, title=\u0026#39;Reference\u0026#39;, ax=ax) ax = fig.add_subplot(132, projection=\u0026#39;3d\u0026#39;) # Draw original data and reference datatools.draw_data_and_ref(data, ref=ref, title=\u0026#39;Raw data\u0026#39;, ax=ax) \u0026hellip;and store the return of the function in the variables T, errors, iterations and total_time as defined by the function definition header in the file icp.py:\ndef icp(data, ref, init_pose=None, max_iterations=20, tolerance=0.001): \u0026#39;\u0026#39;\u0026#39; The Iterative Closest Point method: finds best-fit transform that maps points A on to points B Input: A: Nxm numpy array of source mD points B: Nxm numpy array of destination mD point init_pose: (m+1)x(m+1) homogeneous transformation max_iterations: exit algorithm after max_iterations tolerance: convergence criteria Output: T: final homogeneous transformation that maps A on to B errors: Euclidean distances (errors) for max_iterations iterations in a (max_iterations+1) vector. distances[0] is the initial distance. i: number of iterations to converge total_time : execution time \u0026#39;\u0026#39;\u0026#39; Model transformation The T transformation matrix from ICP is the homogeneous passage matrix that allows us to map the data model, passed as a parameter to the icp function, onto the ref model. As a reminder, the application of a homogeneous matrix to transform a set of points from an initial reference frame \\(\\mathcal{R_i}\\) to a final reference frame \\(\\mathcal{R_f}\\) is performed as follows:\n$$P_f^{(4 \\times N)} = T^{(4 \\times 4)} . P_i^{(4 \\times N)}$$\nIn the code, the third part consists in applying the transformation matrix to the model to be compared. An example of how to apply a homogeneous transformation to the matrix can be written as follows:\n# EXAMPLE of how to apply a homogeneous transformation to a set of points \u0026#39;\u0026#39;\u0026#39; # (1) Make a homogeneous representation of the model to transform homogeneous_model = np.ones((original_model.shape[0], 4)) ##### Construct a [N,4] matrix homogeneous_model[:,0:3] = np.copy(original_model) ##### Replace the X,Y,Z columns with the model points # (2) Construct the R|t homogeneous transformation matrix / here a rotation of 36 degrees around x axis theta = np.radians(36) c, s = np.cos(theta), np.sin(theta) homogeneous_matrix = np.array([[1, 0, 0, 0], [0, c, s, 0], [0, -s, c, 0], [0, 0, 0, 1]]) # (3) Apply the transformation transformed_model = np.dot(homogeneous_matrix, homogeneous_model.T).T # (4) Remove the homogeneous coordinate transformed_model = np.delete(transformed_model, 3, 1) The original variable is an array of size \\(N \\times 3\\), \\(N\\) being the number of points of the model and 3 its coordinates \\(X\\), \\(Y\\) and \\(Z\\).\nYou need to add a homogeneous coordinate and apply the necessary transpositions for the matrix multiplication to work. Use the example given in the code to perform this step.\nYou can then display the result by uncommenting and completing the line datatools.draw_data....\nError display Uncomment and display the error in the last part of the code, changing the \u0026ldquo;\u0026hellip;\u0026rdquo; to the corresponding variables:\n# Display error progress over time # **************** To be uncommented and completed **************** # fig1 = plt.figure(2, figsize=(20, 3)) # it = np.arange(0, len(errors), 1) # plt.plot(it, ...) # plt.ylabel(\u0026#39;Residual distance\u0026#39;) # plt.xlabel(\u0026#39;Iterations\u0026#39;) # plt.title(\u0026#39;Total elapsed time :\u0026#39; + str(...) + \u0026#39; s.\u0026#39;) # plt.show() Step 3: Models comparison Compare the application of ICP on the models data02 and data03, notice the evolution of the error and the differences in values. What does this error represent? What can we say about the two models? Based on the errors, what decision threshold could you choose to determine whether a model is faulty or not?\nICP in CloudCompare The ICP algorithm can also be used directly in CloudCompare. Open it and import data01_segmented.xyz, data02_object.xyz and data03_object.xyz.\nSelect for example the clouds of data01_segmented and data02_object, use the Tools \u0026gt; Registration \u0026gt; Fine registration (ICP) tool. Make sure the reference is data01 and apply ICP. Running it returns the transformation matrix calculated by the algorithm, and applies it to the object.\nWe can then, still selecting the two clouds, calculate the distance between the points with Tools \u0026gt; Distance \u0026gt; Cloud/Cloud Distance. Make sure that the reference is data01 and click on OK/Compute/OK. Select data02_object and display the histogram of its distances to the reference cloud via Edit \u0026gt; Scalar fields \u0026gt; Show histogram.\nDo the same thing with data03_object and compare the histograms. How do you interpret them? How can you compare them?\nCreated by our teacher Claire Labit-Bonis.\n","date":"October 3, 2022","hero":"/posts/ai/computer-vision/3d_perception/cc_segmentation/featured.png","permalink":"https://noegracia.github.io/posts/ai/computer-vision/3d_perception/cc_segmentation/","summary":"3D objects quality control.","tags":["Teaching","3D Perception"],"title":"3DP-TP-00 | 3D segmentation and object shape checking based on RGB-D sensor with CloudCompare"},{"categories":null,"contents":" This exercise consists in filling empty functions, or parts of the localisation.py file:\nstep 1 : perspective_projection and transform_and_draw_model, step 2 : calculate_normal_vector and calculate_error step 3 : partial_derivatives, step 4 : transformation towards a second point of view. This post describes the role of each function.\nDo not dive headfirst into the code ! The first part \u0026ldquo;goal description\u0026rdquo; is just a global and theoretical presentation of the subject. Each step and its associated functions are described in details within their respective parts: step 1, step 2, step 3, step 4. Contents download Practical session files can be downloaded through your course on Moodle, or at this link.\nGoal description This exercise aims at finding the optimal transformation between a 3D model expressed in centimeters in an object coordinate system \\(\\mathcal{R_o} (X,Y,Z)\\) (sometimes called world coordinate system \\(\\mathcal{R_w}\\)) in order to draw it as an overlay in a 2D pixel image defined in its image coordinate system \\(\\mathcal{R_i} (u,v)\\). The 3D model was downloaded from free3d and modified within Blender for the needs of this exercise. Points and edges were respectively exported via a Python script to the pikachu.xyz and pikachu.edges files.\npikachu.xyz contains 134 lines and 3 columns, corresponding to the 134 model points and their three \\(x, y, z\\) coordinates.\npikachu.edges contains 246 lines and 2 columns, corresponding to the 246 model edges and their 2 ends indices (for instance, the first line in pikachu.edges describes the first edge: its first point coordinates are at index 2 in the pikachu.xyz file, and its second point is at index 0).\nBy \u0026ldquo;transformation\u0026rdquo;, we mean scene rotation and translation along the \\(x\\), \\(y\\) and \\(z\\) axes for each point in \\(\\mathcal{R_o}\\).\nIn our case, we want to perform augmented reality by positioning Pikachu\u0026rsquo;s model over the sky-blue cube of the image below, and making it perfectly match with the virtual cube on which it is standing.\nIntrinsic parameters. Click to expand We use the pinhole camera model allowing to perform this operation through two successive transformations: \\(\\mathcal{R_o} \\rightarrow \\mathcal{R_c}\\) and \\(\\mathcal{R_c} \\rightarrow \\mathcal{R_i}\\). Apart from \\(\\mathcal{R_o}\\) and \\(\\mathcal{R_i}\\) coordinate systems, we then must also considerate the camera frame \\(\\mathcal{R_c}\\).\nThe resource Modélisation et calibrage d\u0026rsquo;une caméra describes the pinhole camera model in details, and can help understanding the exercise.\nWikipedia defines the pinhole model as describing the mathematical relationship between the coordinates of a point in three-dimensional space and its projection onto the image plane of an ideal pinhole camera, where the camera aperture is described as a point and no lenses are used to focus light.\nThe change between \\(\\mathcal{R_c}\\) and \\(\\mathcal{R_i}\\) coordinate systems is done thanks to the intrinsic camera parameters. These \\((\\alpha_u, \\alpha_v, u_0, v_0)\\) coefficients are then stored in a homogeneous transformation matrix \\(K_{i \\leftarrow c}\\) so that we can describe the relation \\(p_i = K_{i \\leftarrow c}.P_c\\) as:\n$$ \\begin{bmatrix} u\\\\v\\\\1 \\end{bmatrix}_{\\mathcal{R}_i} = s. \\begin{bmatrix} \\alpha_u \u0026amp; 0 \u0026amp; u_0\\\\ 0 \u0026amp; \\alpha_v \u0026amp; v_0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} . \\begin{bmatrix} X\\\\ Y\\\\ Z \\end{bmatrix}_{\\mathcal{R}_c} $$\nwith:\n\\(p_i\\) (on the left) the pixel point within the 2D image frame \\(\\mathcal{R_i}\\), \\(P_c\\) (on the right) the point in centimeters within the 3D camera frame \\(\\mathcal{R_c}\\), \\(s = \\frac{1}{Z}\\), \\(\\alpha_u = k_x f\\), \\(\\alpha_v = k_y f\\): \\(k_x = k_y\\) the sensor number of pixels per millimeter along \\(x\\) and \\(y\\) directions \u0026ndash; the equality being true only if the pixels are square, \\(f\\) the focal distance. \\(u_0\\) and \\(v_0\\) the image centers in pixels within \\(\\mathcal{R}_i\\). In our case, the image was captured by a Canon EOS 700D sensor of size \\(22.3\\times14.9 mm\\). The image size being \\(720\\times480 px\\) and the focal distance \\(18mm\\), we deduce the parameters \\(\\alpha_u = 581.1659\\), \\(\\alpha_v = 579.8657\\), \\(u_0 = 360\\) and \\(v_0 = 240\\) stored in the calibration_parameters.txt file.\nExtrinsic parameters. Click to expand To reach our goal i.e., display our 3D model in the 2D image in pixels, we have to estimate the \\(\\mathcal{R_o}\\rightarrow\\mathcal{R_c}\\) transformation coefficients and establish the relation \\(P_c=M_{c\\leftarrow o}.P_o\\), with:\n\\(M_{c\\leftarrow o}=\\big[ R_{\\alpha\\beta\\gamma} | T \\big]\\) the homogeneous transformation matrix of the Euler angles and \\(x\\), \\(y\\) and \\(z\\) translation of the coordinate system.\n\\(R_{\\alpha\\beta\\gamma}\\) the rotation matrix resulting from the successive applications \u0026ndash; and thus the multiplication between them \u0026ndash; of the rotation matrices \\(R_\\gamma\\), \\(R_\\beta\\) and \\(R_\\alpha\\):\n$$R_\\alpha = \\begin{bmatrix}1 \u0026amp; 0 \u0026amp; 0\\\\0 \u0026amp; \\cos \\alpha \u0026amp; -\\sin \\alpha\\\\0 \u0026amp; \\sin \\alpha \u0026amp; \\cos \\alpha\\end{bmatrix}$$\n$$R_\\beta = \\begin{bmatrix}\\cos \\beta \u0026amp; 0 \u0026amp; -\\sin \\beta\\\\0 \u0026amp; 1 \u0026amp; 0\\\\\\sin \\beta \u0026amp; 0 \u0026amp; \\cos \\beta\\end{bmatrix}$$\n$$R_\\gamma = \\begin{bmatrix}\\cos \\gamma \u0026amp; -\\sin \\gamma \u0026amp; 0\\\\\\sin \\gamma \u0026amp; \\cos \\gamma \u0026amp; 0\\\\0 \u0026amp; 0 \u0026amp; 1\\end{bmatrix}$$\nWith the correct extrinsic parameters \\((\\alpha, \\beta, \\gamma, t_x, t_y, t_z)\\), the 3D model in its coordinate system \\(\\mathcal{R_o}\\) can be transformed into \\(\\mathcal{R_c}\\) then \\(\\mathcal{R_i}\\), while respecting the following relationship for each point \\(P_o\\):\n$$ p_i = K_{i \\leftarrow c} M_{c\\leftarrow o} P_o$$\n$$\\begin{bmatrix} u\\\\ v\\\\ 1 \\end{bmatrix}_{\\mathcal{R}_i} = s. \\begin{bmatrix} \\alpha_u \u0026amp; 0 \u0026amp; u_0\\\\ 0 \u0026amp; \\alpha_v \u0026amp; v_0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\bigodot \\begin{bmatrix} r_{11} \u0026amp; r_{12} \u0026amp; r_{13} \u0026amp; t_x\\\\ r_{21} \u0026amp; r_{22} \u0026amp; r_{23} \u0026amp; t_y\\\\ r_{31} \u0026amp; r_{32} \u0026amp; r_{33} \u0026amp; t_z\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} . \\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ 1 \\end{bmatrix}_{\\mathcal{R}_o} $$\nThe \\(\\bigodot\\) operator is the multiplication between terms after having removed the homogeneous coordinate from \\(M_{c\\leftarrow o} P_o\\). It is only here so that matrices shapes match. The instrinsic parameters are known; in order to find the extrinsic parameters, we use a localization method (i.e. of parameters estimation) called Perspective-n-Lines.\nExtrinsic parameters estimation Our starting point is an initial coarse estimate of \\((\\alpha, \\beta, \\gamma, t_x, t_y, t_z)\\). This initial estimate is more or less accurate depending on our a priori knowledge of the scene, the object and the position of the camera.\nIn our case, the initial parameters have values \\(alpha = -2.1\\), \\(beta = 0.7\\), \\(gamma = 2.7\\), \\(t_x = 3.1\\), \\(t_y = 1.3\\) and \\(t_z = 18\\), i.e., we know before we even start that the 3D model will have to undergo a rotation of angles \\((-2.1, 0.7, 2.7)\\) around its three axes, and that it should shift approximately \\(3cm\\) in \\(x\\), \\(1.5cm\\) in \\(y\\) and \\(18cm\\) in \\(z\\).\nThis a priori knowledge of the environment comes from the fact that one is able, as a human being, to evaluate the distance of the real object from the sensor only by looking at the image.\nUse of the programme When the script localisation.py is launched, two figures open: one represents a real scene captured by the camera, the other represents the virtual model to be transformed and whose system origin is materialised by a red dot. As the objective is to copy the box on which the Pikachu rests on top of the sky-blue cube on the table, you must first select five edges belonging to the real box (by clicking the mouse on the edges ends), then select the corresponding edges in the same order on the 3D model (by clicking the mouse in the middle of the edges). The number of segments to be selected (by default 5), is fixed from the start in the variable nb_segments.\nIn this way, it will be possible to calculate the error on the estimation of extrinsic parameters by comparing the distance between the edges selected in the image and those of the transformed model. This error criterion is described at step 2.\nStep 1: Display the pattern in \\(\\mathcal{R_i}\\) In the main program, the points of the model are stored in model3d_Ro, a matrix of size \\([246\\times6]\\) corresponding to the 246 edges of the model, each defined by the 6 coordinates of its two points \\(P_1(x_1, y_1, z_1)\\) and \\(P_2(x_2, y_2, z_2)\\). The transformation matrix \\(M_{c\\leftarrow o}\\) is stored in extrinsic_matrix and the intrinsic parameters of the camera are stored in intrinsic_matrix.\nTo visualise the 3D model in the 2D image and get an idea of the accuracy of our estimate, we have to code the tranform_and_draw_model function which allows to apply the \\(K_{i \\leftarrow c} M_{c\\leftarrow o}\\) transformation at each point \\(P_o\\) of a set of edges edges_Ro, with an intrinsic matrix, and an extrinsic matrix to finally display the result in a fig_axis figure:\ndef transform_and_draw_model(edges_Ro, intrinsic, extrinsic, fig_axis): # ********************************************************************* # # TO BE COMPLETED. # # FUNCTIONS YOU WILL HAVE TO USE : # # - perspective_projection # # - transform_point_with_matrix # # Input: # # edges_Ro : ndarray[Nx6] # # N = number of edges in the model # # 6 = (X1, Y1, Z1, X2, Y2, Z2) the P1 and P2 point # # coordinates for each edge # # intrinsic : ndarray[3x3] - camera intrinsic parameters # # extrinsic : ndarray[4x4] - camera extrinsic parameters # # fig_axis : figure used for display # # Output: # # No return - the function only calculates and displays the # # transformed points (u1, v1) and (u2, v2) # # ********************************************************************* # # Part to replace # u_1 = np.zeros((edges_Ro.shape[0],1)) u_2 = np.zeros((edges_Ro.shape[0],1)) v_1 = np.zeros((edges_Ro.shape[0],1)) v_2 = np.zeros((edges_Ro.shape[0],1)) ############### for p in range(edges_Ro.shape[0]): fig_axis.plot([u_1[p], u_2[p]], [v_1[p], v_2[p]], \u0026#39;k\u0026#39;) The objective is to store in the points [u_1, v_1] and [u_2, v_2] the coordinates \\((u, v)\\) of the points \\(P_1\\) and \\(P_2\\) of the edges after transformation.\nThis function can be divided into two sub-steps:\nthe transformation \\(\\mathcal{R_o} \\rightarrow \\mathcal{R_c}\\) of the points \\(P_o\\) using the function transform_point_with_matrix provided in the matTools library:\nP_c = matTools.transform_point_with_matrix(extrinsic, P_o) the projection \\(\\mathcal{R_c} \\rightarrow \\mathcal{R_i}\\) of the newly obtained \\(P_c\\) points using the function perspective_projection which must be completed :\ndef perspective_projection(intrinsic, P_c): # ***************************************************** # # TO BE COMPLETED. # # Useful function: # # np.dot # # Input: # # intrinsic : ndarray[3x3] - intrinsic parameters # # P_c : ndarray[Nx3], # # N = number of points to transform # # 3 = (X, Y, Z) the points coordinates # # Output: # # u, v : two ndarray[N] containing the Ri coordinates # # of the transformed Pc points # # ***************************************************** # u, v = 0, 0 # Part to replace return u, v Once perspective_projection and transform_and_draw_model have been completed, the launch of the overall programme will project the model in the image, with the extrinsic parameters defined at the outset.\nThe transformation applied to these points is obviously not very good, the model does not match the box as one would wish. In order to be able to differentiate a \u0026ldquo;bad\u0026rdquo; estimate of the extrinsic parameters from a \u0026ldquo;good\u0026rdquo; one, and thus be able to automatically propose a new estimate closer to our objective, we must determine an error criterion characterising the distance we are from this optimal objective.\nStep 2: Determine an error criterion The error criterion is used to assess the accuracy of our estimate. The greater the error, the poorer our extrinsic parameters. Once we have determined this error criterion, we can integrate it into an optimisation loop aimed at minimising it, and thus have the best possible extrinsic parameters for our 2D/3D matching objective.\nAs an example, the figure below illustrates this optimisation loop for the projection of the virtual box on the real cube. At iteration \\(0\\), the parameters are coarse, the error is large. By changing the parameters the error will be reduced until ideally zero error and optimal transformation for a perfect projection of the 3D model into the 2D image is achieved.\nIn 2D/3D segment matching, the error criterion to be minimised is the scalar product of the normal to the interpretation plane for the matching. In other words, the objective is to transform the points of the model so that the segments selected in the image and those selected in the model belong to the same plane expressed in the camera frame \\(\\mathcal{R_c}\\).\nAs shown in the figure above, the interpretation plane can be defined as being formed by the segments \\(\\mathcal{l_{i \\rightarrow c}^{j,1}}\\) and \\(\\mathcal{l_{i \\rightarrow c}^{j,2}}\\). In this notation, \\(j\\) corresponds to the number of edges selected when the program is launched (5 by default). For each selected edge \\(j\\) and expressed in the image frame \\(\\mathcal{R_i}\\), there are two segments \\(l^1\\) and \\(l^2\\). The \\(l^1\\) segment is composed of the two ends \\(P_{i \\rightarrow c}^0\\) and \\(P_{i \\rightarrow c}^1\\); the \\(l^2\\) segment is composed of the two ends \\(P_{i \\rightarrow c}^1\\) and \\(P_{i \\rightarrow c}^2\\). Each of the \\(P_{i \\rightarrow c}\\) is a point in the image frame \\(\\mathcal{R_i}\\) transformed into the camera frame \\(\\mathcal{R_c}\\). They are known: they are the ones that have been selected by mouse click.\nOnce these segments have been calculated, we can calculate the normal to the interpretation plane \\(N_c^j\\). As a reminder:\n$$N = \\frac{l^1 \\wedge l^2}{||l^1 \\wedge l^2||}$$\nIn the segment selection function utils.select_segments(), as you make edge selections, the normals are calculated and stored in the normal_vectors matrix thanks to the calculate_normal_vector function which must be completed in the localisation.py file:\ndef calculate_normal_vector(p1_Ri, p2_Ri, intrinsic): # ********************************************************* # # TO BE COMPLETED. # # Useful functions: # # np.dot, np.cross, np.linalg.norm, np.linalg.inv # # Input: # # p1_Ri : list[3] # # 3 = (u, v, 1) of the first selected point # # p2_Ri : list[3] # # 3 = (u, v, 1) of the second selected point # # intrinsic : ndarray[3x3] of intrinsic # # Output: # # normal_vector : ndarray[3] containing the normal to # # L1_c and L2_c segments, deducted from # # the selected image points # # ********************************************************* # normal_vector = np.zeros((len(p1_Ri),)) # Part to replace return normal_vector Then, for each segment \\(j \\in [1, \u0026hellip;, 5]\\), the distance between the plane linked to the image segments and the points of the 3D model can be calculated using the scalar product of the \\(N_c^j\\) and the \\(P_{o\\rightarrow c}^j\\). If the scalar product is null, both vectors are orthogonal and the point \\(P_{o\\rightarrow c}^j\\) belongs to the same plane as \\(P_{i \\rightarrow c}^j\\). The larger the scalar product, the further the extrinsic parameters move away from the correct transformation.\nTo summarize, the parameter optimization criterion is \\(\\sum_{j=1}^{2n} F^j(X)^2\\) (the square removes negative values), with \\(F^j(X) = N^{j ; \\text{mod} ; 2}.P_ {o\\rightarrow c}^{j ; \\text{mod} ; 2, 1|2}\\), depending on whether one is on the point \\(P^{1}\\) or \\(P^{2}\\).\nIn the sum that runs through the points from \\(j\\) to \\(2n\\), \\(j ; \\text{mod} ; 2\\) is the segment index for the current point. In other words, we cumulate the \\((N^i.P^{i, 1})^2\\) and \\((N^i.P^{i, 2})^2\\) for all \\(i\\) segments.\nHere, \\(X\\) designates the set of parameters \\((\\alpha, \\beta, \\gamma, t_x, t_y, t_z)\\) and not a coordinate. Remember that every \\(P_{o\\rightarrow c} = \\big[ R_{\\alpha\\beta\\gamma} | T \\big]. P_o\\) ; the value of the error thus actually depends on the value of the extrinsic parameters. Each time the optimization loop is run, changing the weights of these parameters will influence the \\(F(X)\\) criterion.\nThe function calculate_error calculates the error criterion. It takes as input the number of selected edges nb_segments, the normal_vectors, and the Rc_segments selected and then transformed by the matrix of extrinsic parameters and expressed in \\(\\mathcal{R_c}\\).\ndef calculate_error(nb_segments, normal_vectors, segments_Rc): # ***************************************************************** # # TO BE COMPLETED. # # Input: # # nb_segments : default 5 = number of selected segments # # normal_vectors : ndarray[Nx3] - normal vectors to the # # interpretation plane of selected segments # # N = number of segments # # 3 = (X,Y,Z) normal coordinates in Rc # # segments_Rc : ndarray[Nx6] = selected segments in Ro # # and transformed in Rc # # N = number of segments # # 6 = (X1, Y1, Z1, X2, Y2, Z2) of points P1 and # # P2 of the transformed edges in Rc # # Output: # # err : float64 - cumulated error of observed vs. expected dist. # # ***************************************************************** # err = 0 for p in range(nb_segments): # Part to replace with the error calculation err = err + 0 err = np.sqrt(err / 2 * nb_segments) return err From a mathematical point of view, the sum is written for \\(j\\) ranging from \\(1\\) to \\(2n\\), i.e., the number of points. From an implementation point of view and given the construction of our variables, it is simpler to express the criterion through a sum running along the edges: \\(\\sum_{j=1}^{n} F^j(X)\\) with \\(F^{j}(X) = F^{j, 1}(X)^2 + F^{j, 2}(X)^2\\).\nThus, \\(F^{j, 1}(X) = N^j.P_{o\\rightarrow c}^{j, 1}\\) and \\(F^{j, 2}(X) = N^j.P_{o\\rightarrow c}^{j, 2}\\).\nStep 3: Estimation of parameters by ordinary least squares method At each iteration \\(k\\), we look for a set of parameters \\(X_{k}(\\alpha, \\beta, \\gamma, t_x, t_y, t_z)\\) such as the criterion \\(F(X)\\) is equal to the criterion for the previous parameter set \\(X_0\\) (before update) incremented by a delta weighted by the Jacobian of the function.\nWe thus seek to solve a system of the form :\n$$F(X) \\approx F(X_0) + J \\Delta X$$\nThe Jacobian \\(J\\) contains on its lines the partial derivatives of the \\(F\\) function for each selected point and according to each of the \\(X\\) parameters. It reflects the trend of the criterion (ascending/descending) and the speed at which it increases or decreases according to the value of each of its parameters.\nSince our objective is to reach a criterion \\(F(X) = 0\\), we translate the problem to be solved by:\n$$ \\begin{align} 0 \u0026amp;= F(X_0) + J \\Delta X \\\\\\ \\Leftrightarrow \\qquad -F(X_0) \u0026amp;= J\\Delta X \\end{align} $$\nThe advantage of working with successive increments is that the increment values are so small in relation to the last iteration that the variation of these parameters can be approximated as 0. As a reminder, the error criterion is expressed as follows for each segment \\(j\\):\n$$ \\begin{align} F^{j, 1}(X) \u0026amp;= N^j.P_{o\\rightarrow c}^{j, 1} \\text{ with } P_{o\\rightarrow c}^{j, 1} = \\big[ R_{\\alpha\\beta\\gamma } | T \\big] . P_o^{j, 1}\\\\\\ F^{j, 2}(X) \u0026amp;= N^{j}.P_{o\\rightarrow c}^{j, 2} \\text{ with } P_{o\\rightarrow c}^{j, 2} = \\big[ R_{\\alpha\\beta\\gamma } | T \\big] . P_o^{j, 2} \\end{align} $$\nBecause of always having \\(\\Delta X \\approx 0\\), the calculation of the Jacobian matrix is considerably simplified, since the partial derivatives \\((\\frac{\\partial F^j}{\\partial \\alpha}, \\frac{\\partial F^j}{\\partial \\beta}, \\frac{\\partial F^j}{\\partial \\gamma}, \\frac{\\partial F^j}{\\partial t_x}, \\frac{\\partial F^j}{\\partial t_y}, \\frac{\\partial F^j}{\\partial t_z})\\) are the same at each iteration.\nThe detail of the derivation is given for the first parameter \\(\\alpha\\), the following are to be demonstrated :\n$$\\begin{align} \\frac{\\partial F^j}{\\partial \\alpha} \u0026amp;= N^j . [R_\\gamma . R_\\beta . \\frac{\\partial R_\\alpha}{\\partial \\alpha} | T] . P_o^j\\\\\\ \\end{align} $$\n\\(R_\\gamma\\) and \\(R_\\beta\\) disappear from the derivation because for \\(\\gamma \\approx 0\\) and \\(\\beta \\approx 0\\), we have:\n$$ R_{\\gamma \\approx 0} = \\begin{bmatrix} \\cos 0 \u0026amp; -\\sin 0 \u0026amp; 0\\\\\\ \\sin 0 \u0026amp; \\cos 0 \u0026amp; 0\\\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = I_3 $$\n$$ R_{\\beta \\approx 0} = \\begin{bmatrix} \\cos 0 \u0026amp; 0 \u0026amp; -\\sin 0\\\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\\\ \\sin 0 \u0026amp; 0 \u0026amp; \\cos 0 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = I_3 $$\n\\(T\\) disappears since \\(t_x\\), \\(t_y\\), \\(t_z \\approx 0\\).\nReminder of derivation rules: \\((\\text{constant } a)\u0026rsquo; \\rightarrow 0 \\text{, } (\\sin)\u0026rsquo; \\rightarrow \\cos \\text{, } (\\cos)\u0026rsquo; \\rightarrow -\\sin\\).\n$$ \\begin{align} \\frac{\\partial F^j}{\\partial \\alpha} \u0026amp;= N^j . [I_3 . I_3 . \\frac{\\partial R_\\alpha}{\\partial \\alpha} | 0] . P_o^j\\\\\\ \u0026amp;= N^j . \\frac{\\partial \\begin{bmatrix}1 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; \\cos (\\alpha \\approx 0) \u0026amp; -\\sin (\\alpha \\approx 0)\\\\\\ 0 \u0026amp; \\sin (\\alpha \\approx 0) \u0026amp; \\cos (\\alpha \\approx 0) \\end{bmatrix}}{\\partial \\alpha} . \\begin{bmatrix}X^j\\\\\\ Y^j\\\\\\ Z^j\\end{bmatrix}_o\\\\\\ \u0026amp;= N^j . \\begin{bmatrix}0 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; -\\sin (\\alpha \\approx 0) \u0026amp; -\\cos (\\alpha \\approx 0)\\\\\\ 0 \u0026amp; \\cos (\\alpha \\approx 0) \u0026amp; -\\sin (\\alpha \\approx 0) \\end{bmatrix} . \\begin{bmatrix}X^j\\\\\\ Y^j\\\\\\ Z^j\\end{bmatrix}_o\\\\\\ \u0026amp;= N^j . \\begin{bmatrix}0 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; 0 \u0026amp; -1\\\\\\ 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} . \\begin{bmatrix}X^j\\\\\\ Y^j\\\\\\ Z^j\\end{bmatrix}_o\\\\\\ \u0026amp;= N^j . \\begin{bmatrix}0\\\\\\ -Z^j\\\\\\ Y^j\\end{bmatrix}_o\\\\\\ \\end{align} $$\nThe reasoning is the same for \\(\\frac{\\partial F^j}{\\partial \\beta}, \\frac{\\partial F^j}{\\partial \\gamma}, \\frac{\\partial F^j}{\\partial t_x}, \\frac{\\partial F^j}{\\partial t_y}, \\frac{\\partial F^j}{\\partial t_z}\\) and we get:\n$$ \\frac{\\partial F^j}{\\partial \\beta} = N^j . \\begin{bmatrix}Z^j\\\\\\ 0\\\\\\ -X^j\\end{bmatrix}_o \\text{, } \\frac{\\partial F^j}{\\partial \\gamma} = N^j . \\begin{bmatrix}-Y^j\\\\\\ X^j\\\\\\ 0\\end{bmatrix}_o $$ $$ \\frac{\\partial F^j}{\\partial t_x} = N_x^j \\text{, } \\frac{\\partial F^j}{\\partial t_y} = N_y^j \\text{, } \\frac{\\partial F^j}{\\partial t_z} = N_z^j $$\nEach of these partial derivatives is to be implemented in the partial_derivatives function:\ndef partial_derivatives(normal_vector, P_c): # ********************************************************************* # # TO BE COMPLETED. # # Input: # # normal_vector : ndarray[3] contains the normal of the segment # # to which P_c belongs # # P_c : ndarray[3] the object point transformed to Rc # # Output: # # partial_derivative : ndarray[6] partial derivative of the criterion # # for each extrinsic parameter # # crit_X0 : float64 - criterion value for the current parameters, # # which will be used as initial value before # # the update and recalculation of the error # # ********************************************************************* # X, Y, Z = P_c[0], P_c[1], P_c[2] partial_derivative = np.zeros((6)) # Part to replace ####### partial_derivative[0] = 0 partial_derivative[1] = 0 partial_derivative[2] = 0 partial_derivative[3] = 0 partial_derivative[4] = 0 partial_derivative[5] = 0 ######################### # ******************************************************************** crit_X0 = normal_vector[0] * X + normal_vector[1] * Y + normal_vector[2] * Z return partial_derivative, crit_X0 The problem can thus be formalised as:\n$$ F = J \\Delta X \\text{ with } F = \\begin{bmatrix}-F^1(X_0) \\\\\\ \\vdots \\\\\\ -F^{2n}(X_0)\\end{bmatrix}_{2n\\times 1} $$ $$ \\text{ and } J = \\begin{bmatrix}\\frac{\\partial F^1}{\\partial \\alpha} \u0026amp; \\frac{\\partial F^1}{\\partial \\beta} \u0026amp; \\frac{\\partial F^1}{\\partial \\gamma} \u0026amp; \\frac{\\partial F^1}{\\partial t_x} \u0026amp; \\frac{\\partial F^1}{\\partial t_y} \u0026amp; \\frac{\\partial F^1}{\\partial t_z}\\\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\\ \\frac{\\partial F^{2n}}{\\partial \\alpha} \u0026amp; \\frac{\\partial F^{2n}}{\\partial \\beta} \u0026amp; \\frac{\\partial F^{2n}}{\\partial \\gamma} \u0026amp; \\frac{\\partial F^{2n}}{\\partial t_x} \u0026amp; \\frac{\\partial F^{2n}}{\\partial t_y} \u0026amp; \\frac{\\partial F^{2n}}{\\partial t_z}\\end{bmatrix}_{2n\\times 6} $$ $$ \\text{ and } \\Delta X = \\begin{bmatrix}\\Delta \\alpha \\\\\\ \\vdots \\\\\\ \\Delta t_z\\end{bmatrix}_{6 \\times 1} $$\n\\(2n\\) is the number of selected points, \\(n\\) is the number of edges (one edge = two ends).\n\\(F\\) is known, \\(J\\) is known, all that remains is to estimate \\(\\Delta X\\) so that the equality \\(F = J \\Delta X\\) is \u0026ldquo;as true as possible\u0026rdquo;. This is done by minimising the distance between the two sides of equality:\n$$ \\begin{align} \\min_{\\Delta X} ||F - J\\Delta X||^2 \u0026amp; \\\\\\ \\Leftrightarrow \\qquad \\frac{\\partial ||F - J\\Delta X||^2}{\\partial \\Delta X} \u0026amp;= 0 \\end{align} $$\nIndeed, if the derivative of the function to be minimized is 0, then the curve of the function has definitely reached a minimum.\nReminder of matrices operations:\n\\((A+B)^T = A^T + B^T\\)\n\\((AB)^T = B^T A^T\\)\n\\(A\\times B \\neq B\\times A\\)\n\\(A^2 = A^T A\\)\nBy developing \\((F - J \\Delta X)^2\\), we arrive at :\n$$ \\begin{align} (F - J \\Delta X)^2 \u0026amp; = (F - J \\Delta X)^T(F - J \\Delta X)\\\\\\ \u0026amp;= (F^T - \\Delta X^T J^T)(F - J \\Delta X)\\\\\\ \u0026amp;= F^TF - F^TJ\\Delta X - \\Delta X^T J^T F + \\Delta X^T J^T J \\Delta X \\end{align} $$\nWe use matrices properties to show \\(F^TJ\\Delta X = ((J\\Delta X)^T F)^T = (\\Delta X^T J^T F)^T\\). Let\u0026rsquo;s consider matrices shapes for both sides of this equality: $$ \\begin{align} F^TJ\\Delta X \u0026amp;\\rightarrow [1\\times2n][2n\\times 6][6\\times 1] \\rightarrow [1\\times 1]\\\\\\ (\\Delta X^T J^T F)^T \u0026amp;\\rightarrow [1\\times6][6\\times 2n][2n\\times 1] \\rightarrow [1\\times 1] \\end{align} $$\nGiven that each of these terms is a \\([1\\times 1]\\) matrix in the end, we can write \\((\\Delta X^T J^T F)^T = \\Delta X^T J^T F\\), and thus \\(F^TJ\\Delta X = \\Delta X^T J^T F\\).\nWe deduce from this: $$ \\begin{align} (F - J \\Delta X)^2 \u0026amp; = F^TF - 2\\Delta X^T J^T F + \\Delta X^T J^T J \\Delta X\\\\\\ \\end{align} $$\nSome rules of derivation:\n\\(\\frac{\\partial AX}{\\partial X} = A^T\\), \\(\\frac{\\partial X^TA^T}{\\partial X} = A^T\\), \\(\\frac{\\partial X^TAX}{\\partial X} = 2AX\\).\n$$ \\begin{align} \\frac{\\partial (F - J \\Delta X)^2}{\\partial \\Delta X} \u0026amp; = -2J^T F + 2 J^T J \\Delta X\\\\\\ \u0026amp;= -J^T F + J^T J \\Delta X\\\\\\ \\Leftrightarrow \\qquad J^T F \u0026amp;= J^T J \\Delta X \\\\\\ (J^TJ)^{-1} J^T F \u0026amp;= \\Delta X \\\\\\ \\end{align} $$\nIn a nutshell:\nminimizing the distance between \\(F\\) and \\(J\\Delta X\\) means that \\(\\frac{\\partial (F - J \\Delta X)^2}{\\partial \\Delta X} = 0\\) the solution is \\(\\Delta X = (J^TJ)^{-1} J^T F\\). The matrix \\(J^+ = (J^TJ)^{-1} J^T\\) is called the pseudo-inverse of \\(J\\).\nIn the Python code, we can thus implement the parameter update in the optimization loop. \\(\\Delta X\\) corresponds to the variable named delta_solution. We can then pass delta_solution to the function matTools.construct_matrix_from_vec which returns a matrix of \\(X\\) increments in the same form as extrinsic (the extrinsic parameter matrix).\nEach element of extrinsic is incremented by multiplying it by delta_extrinsic.\n# ********************************************************************* # # TO BE COMPLETED. # # delta_solution = ... # # delta_extrinsic = matTools.construct_matrix_from_vec(delta_solution) # # extrinsic = ... # # ********************************************************************* # Once the optimisation loop is operational, the model transformation can be visualised without the virtual box by removing the first 12 points of model3D_Ro :\nStep 4: projection of the estimated pose on the image taken from a different point of view A second photo has been captured from a different point of view, and the passage matrix between the first and second views is stored and loaded at the beginning of the programme from the corresponding .txt file. This allows the model to be re-projected into the image from the second camera, and the result to be displayed:\nfig5 = plt.figure(5) ax5 = fig5.add_subplot(111) ax5.set_xlim(0.720) ax5.set_ylim(480) plt.imshow(image_2) # To be completed with the passage matrix from Ro to Rc2 # Ro -\u0026gt; Rc and Rc -\u0026gt; Rc2 Ro_to_Rc2 = ... transform_and_draw_model(model3D_Ro[12:], intrinsic_matrix, Ro_to_Rc2, ax5) plt.show(block = False) Bonus for the end Thanks to the estimated extrinsic parameter matrix, and as long as the origin of the object reference does not change, elements can be added to the 3D scene and projected into the image in an identical way. model3D_Ro_final contains the points of a scene with Pikachu and a dinosaur in it.\nThe last lines of the display part are uncommented to get the final result:\nfig6 = plt.figure(6) ax6, lines = utils.plot_3d_model(model3D_Ro_final, fig6) fig7 = plt.figure(7) ax7 = fig7.add_subplot(111) ax7.set_xlim(0, 720) plt.imshow(image) transform_and_draw_model(model3D_Ro_final[12:], intrinsic_matrix, extrinsic_matrix, ax7) plt.show(block=True) Created by our teacher Claire Labit-Bonis.\n","date":"July 16, 2022","hero":"/posts/ai/computer-vision/3d_perception/monocular_localization/featured.png","permalink":"https://noegracia.github.io/posts/ai/computer-vision/3d_perception/monocular_localization/","summary":"Iterative estimation of a camera extrinsic parameters.","tags":["Teaching","3D Perception"],"title":"3DP-TP-01 | Monocular localization with iterative PnL"},{"categories":["Basic"],"contents":"Creation of pre-trained autoencoder to learn the initial condensed representation of unlabeled datasets. This architecture consists of 3 parts:\nEncoder: Compresses the input data from the train-validation-test set into a coded representation which is typically smaller by several orders of magnitude than the input data. Latent Space: This space contains the compressed knowledge representations and is thus the most crucial part of the network. Decoder: A module that helps the network to \u0026ldquo;decompress\u0026rdquo; the knowledge representations and reconstruct the data from their coded form. The output is then compared to a ground truth. Imports from time import time import numpy as np import keras.backend as K from keras.layers import Dense, Input, Layer, InputSpec, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Conv2DTranspose from keras.models import Model from keras.initializers import VarianceScaling from sklearn.cluster import KMeans from sklearn.cluster import MiniBatchKMeans from sklearn import metrics from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt from sklearn.manifold import TSNE from sklearn.decomposition import PCA Loading the data from keras.datasets import mnist from keras.datasets import fashion_mnist import numpy as np # Chargement et normalisation (entre 0 et 1) des données de la base de données MNIST (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255. x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255. x_train = np.reshape(x_train, (len(x_train), 784)) x_test = np.reshape(x_test, (len(x_test), 784)) Classic Autoencoder # Dimension de l\u0026#39;entrée input_img = Input(shape=(784,)) # Dimension de l\u0026#39;espace latent : PARAMETRE A TESTER !! latent_dim = 10 # Définition du encodeur x0 = Dense(500, activation=\u0026#39;relu\u0026#39;)(input_img) x = Dense(200, activation=\u0026#39;relu\u0026#39;)(x0) encoded = Dense(latent_dim, activation=\u0026#39;relu\u0026#39;)(x) # Définition du décodeur decoder_input = Input(shape=(latent_dim,)) x = Dense(200, activation=\u0026#39;relu\u0026#39;)(decoder_input) x1 = Dense(500, activation=\u0026#39;relu\u0026#39;)(x) decoded = Dense(784, activation=\u0026#39;relu\u0026#39;)(x1) # Construction d\u0026#39;un modèle séparé pour pouvoir accéder aux décodeur et encodeur encoder = Model(input_img, encoded) decoder = Model(decoder_input, decoded) # Construction du modèle de l\u0026#39;auto-encodeur encoded = encoder(input_img) decoded = decoder(encoded) autoencoder = Model(input_img, decoded) Summary # Autoencodeur autoencoder.compile(optimizer=\u0026#39;Adam\u0026#39;, loss=\u0026#39;mse\u0026#39;) autoencoder.summary() print(encoder.summary()) print(decoder.summary()) Training autoencoder.fit(x_train, x_train, epochs=20, batch_size=128, shuffle=True, validation_data=(x_test, x_test)) Evaluation # Encode and decode some digits # Note that we take them from the *test* set encoded_imgs = encoder.predict(x_test) decoded_imgs = decoder.predict(encoded_imgs) Visualization n = 10 # How many digits we will display plt.figure(figsize=(20, 4)) for i in range(n): # Display original ax = plt.subplot(2, n, i + 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # Display reconstruction ax = plt.subplot(2, n, i + 1 + n) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() Display # Affichage count=1000 idx = np.random.choice(len(x_test), count) inputs = x_test[idx] coordsAC = encoder.predict(inputs) coordsTSNE = TSNE(n_components=2).fit_transform(inputs.reshape(count, -1)) coordsPCA = PCA(n_components=2).fit_transform(inputs.reshape(count, -1)) classes = y_test[idx] fig, ax = plt.subplots(figsize=(10, 7)) ax.set_title(\u0026#34;Espace latent\u0026#34;) plt.scatter(coordsAC[:, 0], coordsAC[:, 1], c=classes, cmap=\u0026#34;Paired\u0026#34;) plt.colorbar() fig2, ax2 = plt.subplots(figsize=(10, 7)) ax2.set_title(\u0026#34;ACP sur espace latent\u0026#34;) plt.scatter(coordsPCA[:, 0], coordsPCA[:, 1], c=classes, cmap=\u0026#34;Paired\u0026#34;) plt.colorbar() fig3, ax3 = plt.subplots(figsize=(10, 7)) ax3.set_title(\u0026#34;tSNE sur espace latent\u0026#34;) plt.scatter(coordsTSNE[:, 0], coordsTSNE[:, 1], c=classes, cmap=\u0026#34;Paired\u0026#34;) plt.colorbar() ","date":"January 10, 2022","hero":"/posts/ai/machine-learning/autoencoder/images/coded-decoded-mnist.jpg","permalink":"https://noegracia.github.io/posts/ai/machine-learning/autoencoder/","summary":"Creation of pre-trained autoencoder to learn the initial condensed representation of unlabeled datasets. This architecture consists of 3 parts:\nEncoder: Compresses the input data from the train-validation-test set into a coded representation which is typically smaller by several orders of magnitude than the input data. Latent Space: This space contains the compressed knowledge representations and is thus the most crucial part of the network. Decoder: A module that helps the network to \u0026ldquo;decompress\u0026rdquo; the knowledge representations and reconstruct the data from their coded form.","tags":["AI","ML","Autoencoder"],"title":"Autoencoder"},{"categories":["Basic"],"contents":"Skin Cancer Detection Tool README.md Overview The objective of this project is to build a Skin Cancer Detection Tool. The tool that we are creating is a segmentation model of spots (moles, melanomas, etc\u0026hellip;) on microscopic images of the skin. To create this tool we will have to train a semantic segmentation AI model. The data that we use for that training is from The International Skin Imaging Collaboration.\nFile Descriptions: data.py: Contains functions to process and load the dataset, preprocess the images, and masks and to create TensorFlow datasets.\nprocess_data(data_path, file_path): Reads the image and mask paths from the dataset. load_data(path): Load training, validation, and test data. read_image(x) and read_mask(x): Read the images and the masks respectively. tf_dataset(x, y, batch=8): Create a TensorFlow dataset. preprocess(x, y): Preprocess the images and masks. predict.py: Uses a pretrained model to make predictions on new images.\nget_data(): Load test images from the INPUT_FOLDER. Then, predictions are made using the loaded model and saved to the OUTPUT_FOLDER. Setup \u0026amp; Requirements Requirements: python 3.x pandas numpy scikit-learn tensorflow 2.x opencv-python You can install these requirements using:\npip install pandas numpy scikit-learn tensorflow opencv-python Steps to Run: Data Preparation:\nPlace your dataset in an appropriate directory. Adjust the paths in the data.py script. Run the data.py script to check if data is loaded properly. python data.py Predicting:\nPlace your test images in the INPUT_FOLDER. Ensure the model path \u0026ldquo;segm_model\u0026rdquo; in predict.py corresponds to your trained model. Run the predict.py script to make predictions. python predict.py Notes This tool currently segments the spots and saves the segmented images in the OUTPUT_FOLDER. You might need to train the model first using your data to get the \u0026ldquo;segm_model\u0026rdquo;. Ensure the directories mentioned in the scripts exist or are modified according to your directory structure. ","date":"August 10, 2021","hero":"/posts/ai/machine-learning/segmentation/images/portada-segm.jpg","permalink":"https://noegracia.github.io/posts/ai/machine-learning/segmentation/","summary":"Skin Cancer Detection Tool README.md Overview The objective of this project is to build a Skin Cancer Detection Tool. The tool that we are creating is a segmentation model of spots (moles, melanomas, etc\u0026hellip;) on microscopic images of the skin. To create this tool we will have to train a semantic segmentation AI model. The data that we use for that training is from The International Skin Imaging Collaboration.\nFile Descriptions: data.","tags":["AI","ML","Autoencoder","Personal Project"],"title":"Skin Cancer Detection using semantic segmentation"},{"categories":null,"contents":"Go Notes ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/_index.bn/","summary":"Go Notes ","tags":null,"title":"Go এর নোট সমূহ"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/posts/ai/computer-vision/3d_perception/_index.fr/","summary":"","tags":null,"title":"Perception 3D"},{"categories":null,"contents":"Bash Notes ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/bash/_index.bn/","summary":"Bash Notes ","tags":null,"title":"ব্যাশের নোট সমূহ"}]