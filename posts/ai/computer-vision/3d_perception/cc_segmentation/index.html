<!doctype html><html lang=en><head><title>3DP-TP-00 | 3D segmentation and object shape checking based on RGB-D sensor with CloudCompare</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.650f674fc19266f4ea6c7b54bb6a4ffad10dfa641e3bea6de0a8472a99807590.css integrity="sha256-ZQ9nT8GSZvTqbHtUu2pP+tEN+mQeO+pt4KhHKpmAdZA="><link rel=icon type=image/png href=/images/site/favicon_hu994095d20d2022b1350b954a89d4b9d4_26338_42x0_resize_box_3.png><meta property="og:title" content="3DP-TP-00 | 3D segmentation and object shape checking based on RGB-D sensor with CloudCompare"><meta property="og:description" content="3D objects quality control."><meta property="og:type" content="article"><meta property="og:url" content="https://noegracia.github.io/posts/ai/computer-vision/3d_perception/cc_segmentation/"><meta property="og:image" content="https://noegracia.github.io/posts/ai/computer-vision/3d_perception/cc_segmentation/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-03T08:06:25+06:00"><meta property="article:modified_time" content="2022-10-03T08:06:25+06:00"><meta name=description content="3D objects quality control."></head><body class="type-posts kind-page" data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hud2c97a9f30beba9b284492c40f2d6757_11039_42x0_resize_q75_box.jpg alt=Logo>
Noé GRACIA</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hud2c97a9f30beba9b284492c40f2d6757_11039_42x0_resize_q75_box.jpg class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hud2c97a9f30beba9b284492c40f2d6757_11039_42x0_resize_q75_box.jpg class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/ai/>Artificial Intelligence</a><ul class=active><li><a class=active href=/posts/ai/computer-vision/ title="Computer Vision">Computer Vision</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/ai/machine-learning/>Machine learning</a><ul><li><a href=/posts/ai/machine-learning/autoencoder/ title=Autoencoder>Autoencoder</a></li><li><a href=/posts/ai/machine-learning/segmentation/ title="Semantic Segmentation">Semantic Segmentation</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/my-life/>My life</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/my-life/about-me/>About me</a><ul><li><a href=/posts/my-life/about-me/my-setup/ title="My setup">My setup</a></li><li><a href=/posts/my-life/about-me/my-routine/ title="My routine">My routine</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/my-life/mountain/>Mountain</a><ul><li><a href=/posts/my-life/mountain/aneto/ title=Aneto>Aneto</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href>teaching</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/ai/computer-vision/3d_perception/>3D Perception</a><ul class=active><li><a class=active href=/posts/ai/computer-vision/3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/posts/ai/computer-vision/3d_perception/monocular_localization/ title="Mono. localization">Mono. localization</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/ai/computer-vision/3d_perception/cc_segmentation/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/noe_huee10e7eb3748b2b35cc484af7cf5c4fa_93893_120x120_fit_q75_box.jpeg alt="Author Image"><h5 class=author-name>Noé Gracia</h5><p>Monday, October 3, 2022</p></div><div class=title><h1>3DP-TP-00 | 3D segmentation and object shape checking based on RGB-D sensor with CloudCompare</h1></div><div class=taxonomy-terms><ul style=padding-left:0><li class=rounded><a href=/tags/teaching/ class="btn, btn-sm">Teaching</a></li><li class=rounded><a href=/tags/3d-perception/ class="btn, btn-sm">3D Perception</a></li></ul></div><div class=post-content id=post-content><h2 id=contents-download>Contents download</h2><p>Practical session files can be downloaded through your course on Moodle, or at <a href=files/files.zip>this link</a>.</p><h2 id=anchor-step-0>Goal description</h2><p>This practical session aims at performing 3D segmentation and shape checking on polyhedral objects, i.e. checking if these objects are correct with respect to a reference geometrical model and/or have defects (holes, residues, etc.).</p><p><img src=images/objets_a_comparer.png alt="Objects to compare" class=center><div style=margin-top:1rem></div></p><p>To do this, we must first build this reference model from an RGB-D image of a defect-free object. Then, for any view of an unknown (also called &ldquo;test&rdquo;) object, we must segment it from the background and compare it with the reference model. This process of checking the shape must be independent of the viewpoint and,
This shape verification process must be viewpoint independent and therefore requires the registration of each associated point cloud against the reference one.</p><div class="alert alert-info"><strong><p>We propose to break this objective down into three steps:</p><ul><li><a href=#anchor-step-1>step 1</a> : <strong>extract the 3D points</strong> of the objects to be compared (both reference and test objects) by deleting all the points of the scene not belonging to the object. To avoid a redundant process, this step will be performed only on the reference scene contained in <code>data01.xyz</code>; this has already been done on the objects to be tested, and stored in the files <code>data02_object.xyz</code> and <code>data03_object.xyz</code>.</li><li><a href=#anchor-step-2>step 2</a> : <strong>register</strong> the points of each test object to the reference model in order to compare them i.e. align their respective 3D point clouds to the reference coordinate frame.</li><li><a href=#anchor-step-3>step 3</a> : <strong>compare</strong> the control and reference models and conclude on the potential flaws of the control models.</li></ul></strong></div><h2 id=step-1-3d-model-extraction-from-the-reference-scene><em><strong>Step 1</strong></em>: 3D model extraction from the reference scene</h2><p>The first step of the practical session consists in extracting the point cloud of the reference model from the RGB-D scene acquired with a Kinect :</p><p><img src=images/extraction_objet.png alt="Reference model extraction" class=center><div style=margin-top:1rem></div></p><p>This step aims at tracing a plane surface on the ground plane, and only keeping the center box by calculating the distance of each of its points from this plane, before applying a filtering threshold.</p><p>To do this, open CloudCompare (the main program, not the viewer) and import the points of the <code>data01.xyz</code> scene. Select the cloud by clicking on it in the workspace.
Using the segmentation tool (<strong>Edit > Segment</strong>, or directly the &ldquo;scissors&rdquo; shortcut in the shortcut bar), divide the cloud into three subsets in order to extract the ground plane and a rough area around the box.
The result is shown in the following figure:</p><p><img src=images/extraction_modele.gif alt="Division of the scene into three clouds" class=center><div style=margin-top:1rem></div></p><div class="alert alert-warning"><strong><p>In CloudCompare, to work on a point cloud, the corresponding line must be selected in the workspace. You know if the cloud is selected when a yellow box is displayed around it.</p><p>Checking the box does not select the cloud, it simply makes it visible/invisible in the display.</p></strong></div><p>Create a surface fitting the ground plane cloud using the <strong>Tools > Fit > Plane</strong> tool.
By selecting the newly created plane and the cloud that contains the box, it is now possible to calculate, for each of the points of this cloud, its distance to the plane using the <strong>Tools > Distances > Cloud/Mesh Distance</strong> tool:</p><p><img src=images/fit_plane_compute_distance.png alt="Plane surface and distance" class=center><div style=margin-top:1rem></div></p><p>The distance tool adds a fourth field to each point of the cloud: the newly calculated distance. Using the cloud properties, filter the points with respect to this scalar field to keep only the points belonging to the box :</p><p><img src=images/filter_by_value.gif alt="Filtering by the distance to the plane" class=center><div style=margin-top:1rem></div></p><p>By clicking on <em>split</em>, two clouds are created, corresponding to the two sides of the filter:</p><p><img src=images/split.png alt="Extracted box" class=center><div style=margin-top:1rem></div></p><p>Make sure that the newly created cloud contains about 10,000 points (the number of points is accessible in the properties panel on the left).</p><p>Only select the box cloud before saving it in ASCII Cloud format as <code>data01_segmented.xyz</code> in your <code>data</code> folder.</p><div class="alert alert-info"><strong>As a precaution, save your CloudCompare project: remember to <strong>select all point clouds</strong>, and save the project in CloudCompare format.</strong></div><h2 id=step-2-3d-points-registration><em><strong>Step 2</strong></em>: 3D points registration</h2><p>If you have opened the complete scenes <code>data02.xyz</code> and <code>data03.xyz</code> in CloudCompare, you will have noticed that each scene was taken from a slightly different point of view, and that the objects themselves have moved:</p><p><img src=images/points_vue_diff.gif alt="Different views of the scenes" class=center><div style=margin-top:1rem></div></p><p>In order to compare the models between them, we propose to overlay them and to calculate their cumulative distance point to point. The smaller this distance, the more the models overlap and resemble each other; the larger it is, the more the models differ.
The following example shows the superposition of the correct model on the previously extracted reference model:</p><p><img src=images/plot_after_icp.png alt="ICP applied to the correct model" class=center><div style=margin-top:1rem></div></p><p>Transforming the points of a model via a rotation/translation matrix to overlay it on another cloud is called <em>point registration</em>.
The <em><strong>Iterative Closest Point</strong></em> algorithm allows this registration, and we propose to use it in Python.
The code to be modified is only in <code>qualitycheck.py</code>, the goal being to apply ICP on both the correct model <code>data02_object.xyz</code>, and on the faulty model <code>data03_object.xyz</code>.</p><h3 id=loading-models>Loading models</h3><p>The first part of the code loads the <code>.xyz</code> models extracted with CloudCompare, stores the reference model in the <code>ref</code> variable and the model to be compared in the <code>data</code> variable.
To run the code on either <code>data02_object</code> or <code>data03_object</code>, just comment out the corresponding line.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Load pre-processed model point cloud</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Extracting MODEL object...&#34;</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> datatools<span style=color:#f92672>.</span>load_XYZ_data_to_vec(<span style=color:#e6db74>&#39;data/data01_segmented.xyz&#39;</span>)[:,:<span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load raw data point cloud</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Extracting DATA02 object...&#34;</span>)
</span></span><span style=display:flex><span>data02_object <span style=color:#f92672>=</span> datatools<span style=color:#f92672>.</span>load_XYZ_data_to_vec(<span style=color:#e6db74>&#39;data/data02_object.xyz&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load raw data point cloud</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Extracting DATA03 object...&#34;</span>)
</span></span><span style=display:flex><span>data03_object <span style=color:#f92672>=</span> datatools<span style=color:#f92672>.</span>load_XYZ_data_to_vec(<span style=color:#e6db74>&#39;data/data03_object.xyz&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ref <span style=color:#f92672>=</span> model_object
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> data02_object
</span></span><span style=display:flex><span><span style=color:#75715e># data = data03_object</span>
</span></span></code></pre></div><h3 id=icp-call>ICP call</h3><p>The second part of the code consists in coding the call to the <code>icp</code> function of the <code>icp</code> library&mldr;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>##########################################################################</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Call ICP:</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#   Here you have to call the icp function in icp library, get its return</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#   variables and apply the transformation to the model in order to overlay</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#   it onto the reference model.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>matrix <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>eye(<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>4</span>)        <span style=color:#75715e># Transformation matrix returned by icp function</span>
</span></span><span style=display:flex><span>errors <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>100</span>))  <span style=color:#75715e># Error value for each iteration of ICP</span>
</span></span><span style=display:flex><span>iterations <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>            <span style=color:#75715e># The total number of iterations applied by ICP</span>
</span></span><span style=display:flex><span>total_time<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>                <span style=color:#75715e># Total time of convergence of ICP</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------- YOUR TURN HERE -------- </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Draw results</span>
</span></span><span style=display:flex><span>fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>1</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>131</span>, projection<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;3d&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Draw reference</span>
</span></span><span style=display:flex><span>datatools<span style=color:#f92672>.</span>draw_data(ref, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Reference&#39;</span>, ax<span style=color:#f92672>=</span>ax)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>132</span>, projection<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;3d&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Draw original data and reference</span>
</span></span><span style=display:flex><span>datatools<span style=color:#f92672>.</span>draw_data_and_ref(data, ref<span style=color:#f92672>=</span>ref, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Raw data&#39;</span>, ax<span style=color:#f92672>=</span>ax)
</span></span></code></pre></div><p>&mldr;and store the return of the function in the variables <code>T</code>, <code>errors</code>, <code>iterations</code> and <code>total_time</code> as defined by the function definition header in the file <code>icp.py</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>icp</span>(data, ref, init_pose<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, max_iterations<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, tolerance<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    The Iterative Closest Point method: finds best-fit transform that maps points A on to points B
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Input:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        A: Nxm numpy array of source mD points
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        B: Nxm numpy array of destination mD point
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        init_pose: (m+1)x(m+1) homogeneous transformation
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        max_iterations: exit algorithm after max_iterations
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        tolerance: convergence criteria
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Output:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        T: final homogeneous transformation that maps A on to B
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        errors: Euclidean distances (errors) for max_iterations iterations in a (max_iterations+1) vector. distances[0] is the initial distance.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        i: number of iterations to converge
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        total_time : execution time
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</span></span></code></pre></div><h3 id=model-transformation>Model transformation</h3><p>The <code>T</code> transformation matrix from ICP is the homogeneous passage matrix that allows us to map the <code>data</code> model, passed as a parameter to the <code>icp</code> function, onto the <code>ref</code> model.
As a reminder, the application of a homogeneous matrix to transform a set of points from an initial reference frame \(\mathcal{R_i}\) to a final reference frame \(\mathcal{R_f}\) is performed as follows:</p><p>$$P_f^{(4 \times N)} = T^{(4 \times 4)} . P_i^{(4 \times N)}$$</p><p>In the code, the third part consists in applying the transformation matrix to the model to be compared. An example of how to apply a homogeneous transformation to the matrix can be written as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># EXAMPLE of how to apply a homogeneous transformation to a set of points</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39; 
</span></span></span><span style=display:flex><span><span style=color:#e6db74># (1) Make a homogeneous representation of the model to transform
</span></span></span><span style=display:flex><span><span style=color:#e6db74>homogeneous_model = np.ones((original_model.shape[0], 4))   ##### Construct a [N,4] matrix
</span></span></span><span style=display:flex><span><span style=color:#e6db74>homogeneous_model[:,0:3] = np.copy(original_model)          ##### Replace the X,Y,Z columns with the model points
</span></span></span><span style=display:flex><span><span style=color:#e6db74># (2) Construct the R|t homogeneous transformation matrix / here a rotation of 36 degrees around x axis
</span></span></span><span style=display:flex><span><span style=color:#e6db74>theta = np.radians(36)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>c, s = np.cos(theta), np.sin(theta)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>homogeneous_matrix = np.array([[1, 0, 0, 0],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                               [0, c, s, 0],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                               [0, -s, c, 0],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                               [0, 0, 0, 1]])
</span></span></span><span style=display:flex><span><span style=color:#e6db74># (3) Apply the transformation
</span></span></span><span style=display:flex><span><span style=color:#e6db74>transformed_model = np.dot(homogeneous_matrix, homogeneous_model.T).T
</span></span></span><span style=display:flex><span><span style=color:#e6db74># (4) Remove the homogeneous coordinate
</span></span></span><span style=display:flex><span><span style=color:#e6db74>transformed_model = np.delete(transformed_model, 3, 1)
</span></span></span></code></pre></div><div class="alert alert-info"><strong><p>The <code>original</code> variable is an array of size \(N \times 3\), \(N\) being the number of points of the model and 3 its coordinates \(X\), \(Y\) and \(Z\).</p><p>You need to add a homogeneous coordinate and apply the necessary transpositions for the matrix multiplication to work.
Use the example given in the code to perform this step.</p></strong></div><p>You can then display the result by uncommenting and completing the line <code>datatools.draw_data...</code>.</p><h3 id=error-display>Error display</h3><p>Uncomment and display the error in the last part of the code, changing the &ldquo;&mldr;&rdquo; to the corresponding variables:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Display error progress over time</span>
</span></span><span style=display:flex><span><span style=color:#75715e># **************** To be uncommented and completed ****************</span>
</span></span><span style=display:flex><span><span style=color:#75715e># fig1 = plt.figure(2, figsize=(20, 3))</span>
</span></span><span style=display:flex><span><span style=color:#75715e># it = np.arange(0, len(errors), 1)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.plot(it, ...)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.ylabel(&#39;Residual distance&#39;)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.xlabel(&#39;Iterations&#39;)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.title(&#39;Total elapsed time :&#39; + str(...) + &#39; s.&#39;)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.show()</span>
</span></span></code></pre></div><h2 id=step-3-models-comparison><em><strong>Step 3</strong></em>: Models comparison</h2><p>Compare the application of ICP on the models <code>data02</code> and <code>data03</code>, notice the evolution of the error and the differences in values.
What does this error represent? What can we say about the two models? Based on the errors, what decision threshold could you choose to determine whether a model is faulty or not?</p><h3 id=icp-in-cloudcompare>ICP in CloudCompare</h3><p>The ICP algorithm can also be used directly in CloudCompare. Open it and import <code>data01_segmented.xyz</code>, <code>data02_object.xyz</code> and <code>data03_object.xyz</code>.</p><p>Select for example the clouds of <code>data01_segmented</code> and <code>data02_object</code>, use the <strong>Tools > Registration > Fine registration (ICP)</strong> tool. Make sure the reference is <code>data01</code> and apply ICP.
Running it returns the transformation matrix calculated by the algorithm, and applies it to the object.</p><p><img src=images/registration_cc.png alt="ICP in CloudCompare" class=center><div style=margin-top:1rem></div></p><p>We can then, still selecting the two clouds, calculate the distance between the points with <strong>Tools > Distance > Cloud/Cloud Distance</strong>. Make sure that the reference is <code>data01</code> and click on OK/Compute/OK.
Select <code>data02_object</code> and display the histogram of its distances to the reference cloud <em>via</em> <strong>Edit > Scalar fields > Show histogram</strong>.</p><p><img src=images/histogram_dists.png alt="Histogram of point to point distances of data02 vs. data01" class=center><div style=margin-top:1rem></div></p><p>Do the same thing with <code>data03_object</code> and compare the histograms. How do you interpret them? How can you compare them?</p><p>Created by our teacher <a href=https://clairelabitbonis.github.io/ target=_blank rel=noopener>Claire Labit-Bonis</a>.</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>Share on:</strong>
<a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f&text=3DP-TP-00%20%7c%203D%20segmentation%20and%20object%20shape%20checking%20based%20on%20RGB-D%20sensor%20with%20CloudCompare&via=No%c3%a9%20GRACIA" target=_blank><i class="fab fa-twitter"></i>
</a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f&title=3DP-TP-00%20%7c%203D%20segmentation%20and%20object%20shape%20checking%20based%20on%20RGB-D%20sensor%20with%20CloudCompare" target=_blank><i class="fab fa-reddit"></i>
</a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f&title=3DP-TP-00%20%7c%203D%20segmentation%20and%20object%20shape%20checking%20based%20on%20RGB-D%20sensor%20with%20CloudCompare" target=_blank><i class="fab fa-linkedin"></i>
</a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=3DP-TP-00%20%7c%203D%20segmentation%20and%20object%20shape%20checking%20based%20on%20RGB-D%20sensor%20with%20CloudCompare https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f" target=_blank><i class="fab fa-whatsapp"></i>
</a><a class="btn btn-sm email-btn" href="mailto:?subject=3DP-TP-00%20%7c%203D%20segmentation%20and%20object%20shape%20checking%20based%20on%20RGB-D%20sensor%20with%20CloudCompare&body=https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/noegracia/noegracia.github.io/edit/main/content/posts/ai/computer-vision/3d_perception/cc_segmentation/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/my-life/mountain/aneto/ title=Aneto class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>Aneto</div></a></div><div class="col-md-6 next-article"><a href=/posts/ai/computer-vision/3d_perception/monocular_localization/ title="3DP-TP-01 | Monocular localization with iterative PnL" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>3DP-TP-01 | Monocular localization with iterative PnL</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#contents-download>Contents download</a></li><li><a href=#anchor-step-0>Goal description</a></li><li><a href=#step-1-3d-model-extraction-from-the-reference-scene><em><strong>Step 1</strong></em>: 3D model extraction from the reference scene</a></li><li><a href=#step-2-3d-points-registration><em><strong>Step 2</strong></em>: 3D points registration</a><ul><li><a href=#loading-models>Loading models</a></li><li><a href=#icp-call>ICP call</a></li><li><a href=#model-transformation>Model transformation</a></li><li><a href=#error-display>Error display</a></li></ul></li><li><a href=#step-3-models-comparison><em><strong>Step 3</strong></em>: Models comparison</a><ul><li><a href=#icp-in-cloudcompare>ICP in CloudCompare</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://noegracia.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://noegracia.github.io/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://noegracia.github.io/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://noegracia.github.io/#education>Education</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:noegraciagirona@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>noegraciagirona@gmail.com</span></a></li><li><a href=https://www.linkedin.com/in/noegracia target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Noé Gracia</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2023 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.710b031badd7b32dd74ed8c97e636fdcdbd226e1fbd0ac60dbfd971c036a640a.js integrity="sha256-cQsDG63Xsy3XTtjJfmNv3NvSJuH70Kxg2/2XHANqZAo=" defer></script></body></html>